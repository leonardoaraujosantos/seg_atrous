{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Search\n",
    "Use Bayesian optimization to find (learning-rate and momentum). The advantage of bayesian optimization compared to grid/random search is that those methods are less sample efficient compared to bayesian optimization.\n",
    "\n",
    "#### References\n",
    "* https://en.wikipedia.org/wiki/Bayesian_optimization\n",
    "* https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f\n",
    "* https://ax.dev\n",
    "* https://ax.dev/tutorials/tune_cnn.html\n",
    "* https://botorch.org\n",
    "* https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf\n",
    "* http://krasserm.github.io/2018/03/21/bayesian-optimization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Number of GPUs Available: 8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Dict, Tuple\n",
    "import torch.utils.data as utils\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Library for augmentations on batch of numpy/tensors\n",
    "from imgaug import augmenters as iaa\n",
    "import lamb as lb\n",
    "\n",
    "import sat_utils\n",
    "import seg_loss\n",
    "import seg_metrics\n",
    "import seg_models\n",
    "\n",
    "from ax.plot.contour import plot_contour\n",
    "from ax.plot.trace import optimization_trace_single_method\n",
    "from ax.service.managed_loop import optimize\n",
    "from ax.utils.notebook.plotting import render, init_notebook_plotting\n",
    "from ax.utils.tutorials.cnn_utils import load_mnist, train, evaluate\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = 'cpu'\n",
    "print('Device:', device)\n",
    "num_gpu = torch.cuda.device_count()\n",
    "print('Number of GPUs Available:', num_gpu)\n",
    "\n",
    "lr=1e-6 #0.001 0.0001-(Good with Dice, 0.007 training)\n",
    "l2_norm=0.0000001\n",
    "gamma=0.1\n",
    "batch_size = 600 #600 #32 #20\n",
    "num_epochs = 50\n",
    "step_size = 200\n",
    "\n",
    "writer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data from pickle (Bad not scalable) and create data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_t: (2038, 3, 76, 76)\n",
      "Y_t: (2038, 1, 76, 76)\n",
      "X_v: (227, 3, 76, 76)\n",
      "Y_v: (227, 1, 76, 76)\n",
      "Num classes: 1\n"
     ]
    }
   ],
   "source": [
    "X = sat_utils.read_pickle_data('./data/input.pickle')\n",
    "Y = sat_utils.read_pickle_data('./data/label.pickle')\n",
    "\n",
    "# Convert dictionaries to numpy array\n",
    "X = np.stack([sat_utils.get_rgb(x) for x in X.values()])\n",
    "#X = np.stack([sat_utils.get_rgbi(x) for x in X.values()])\n",
    "Y = np.stack([(x/255.0) for x in Y.values()])\n",
    "#Y = np.stack([(x) for x in Y.values()])\n",
    "# Select only mask and between buildings\n",
    "#Y = np.stack([Y[:, 0, :, :], Y[:, 1, :, :]], axis=1)\n",
    "# Select only mask\n",
    "Y = np.expand_dims(Y[:, 0, :, :], axis=1)\n",
    "\n",
    "# Split train/validation\n",
    "X_t, X_v, Y_t, Y_v = train_test_split(X, Y, test_size=1/10, random_state=42)\n",
    "print('X_t:', X_t.shape)\n",
    "print('Y_t:', Y_t.shape)\n",
    "print('X_v:', X_v.shape)\n",
    "print('Y_v:', Y_v.shape)\n",
    "\n",
    "# Changes on label for Cross-Entropy (3 classes all mixed on the same image, N,W,H)\n",
    "# Changes on label for BCEWithLogitsLoss (3 classes on 3 Channels, N,C,W,H)\n",
    "tensor_x_t = torch.Tensor(X_t)\n",
    "tensor_y_t = torch.Tensor(Y_t)\n",
    "tensor_x_v = torch.Tensor(X_v)\n",
    "tensor_y_v = torch.Tensor(Y_v)\n",
    "\n",
    "# Define some augmentations\n",
    "seq_augm = iaa.Sequential([\n",
    "    iaa.Fliplr(0.5), # horizontally flip 50% of the images\n",
    "    iaa.Flipud(0.5), # vertically flip 50% of the images\n",
    "    #iaa.Affine(rotate=(-10, 10)), # Rotate the images\n",
    "])\n",
    "\n",
    "dataset_train = utils.TensorDataset(tensor_x_t,tensor_y_t)\n",
    "dataset_val = utils.TensorDataset(tensor_x_v,tensor_y_v)\n",
    "dataloader_train = utils.DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "dataloader_val = utils.DataLoader(dataset_val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "num_classes = tensor_y_t.shape[1]\n",
    "print('Num classes:', num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Train Function\n",
    "Train the model for one epoch given:\n",
    "* Train loader\n",
    "* Parameters\n",
    "* Validation loader (Just to debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader: DataLoader, val_loader: DataLoader, parameters: Dict[str, float], dtype: torch.dtype, device: torch.device) -> nn.Module:\n",
    "    # Start Tensorboard interface\n",
    "    global writer\n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    # Initialize Model and optimizer\n",
    "    model = seg_models.AtrousSeg(num_classes=num_classes, num_channels=tensor_x_t.shape[1])\n",
    "    \n",
    "    # Split bath into multiple GPUs (If available)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    # Load parameters from Ax Dictionary\n",
    "    base_lr = parameters.get(\"lr\", 0.001)\n",
    "    lw_bce = parameters.get(\"bce_weight\", 1.0)\n",
    "    lw_iou = parameters.get(\"iou_weight\", 1.0)\n",
    "    lw_dice = parameters.get(\"dice_weight\", 1.0)\n",
    "    # Print parameters of loss weight\n",
    "    writer.add_text('Params', \n",
    "                    'bce_weight:' + str(lw_bce) + ' iou_weight:' + str(lw_iou) + ' dice_weight:' + str(lw_dice) + ' base_lr:' + str(base_lr), \n",
    "                    0)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    #optimizer = optim.SGD(model.parameters(), lr=parameters.get(\"lr\", 0.001), momentum=parameters.get(\"momentum\", 0.9))\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=base_lr, weight_decay=l2_norm)\n",
    "    optimizer = lb.Lamb(model.parameters(), lr=base_lr, weight_decay=0.01, betas=(.9, .99), adam=True)\n",
    "    criterion = nn.BCELoss()    \n",
    "    #criterion = nn.MSELoss()\n",
    "    #criterion = nn.SmoothL1Loss()\n",
    "    \n",
    "    # Decrease learning rate if some metric doesnt change for \"patience\" epochs\n",
    "    scheduler_plateau = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=4, verbose=False)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # Train Network\n",
    "    iteration_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for (imgs, labels) in train_loader:\n",
    "\n",
    "            # Do augmentations the augmentation library expect numpy arrays on format Batch x Row x Cols x Channels\n",
    "            imgs_aug, labels_aug = seq_augm(images=np.moveaxis(imgs.numpy(),1,3), heatmaps=np.moveaxis(labels.numpy(),1,3))\n",
    "\n",
    "            # Move axis back and convert back to tensor\n",
    "            imgs = torch.from_numpy(np.moveaxis(imgs_aug,3,1))\n",
    "            labels = torch.from_numpy(np.moveaxis(labels_aug,3,1))\n",
    "\n",
    "            # Send inputs/labels to GPU                \n",
    "            labels = labels.to(device)\n",
    "            imgs = imgs.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(imgs)\n",
    "            dice_value  = seg_loss.dice_loss(outputs, labels)\n",
    "            ioU_value  = seg_loss.iou_loss(outputs, labels)\n",
    "            bce_value = criterion(outputs, labels)\n",
    "            loss = (lw_bce*bce_value) + (lw_dice*dice_value) + (lw_iou*ioU_value)\n",
    "            #loss = bce_value\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            writer.add_scalar('loss/', loss.item(), iteration_count)\n",
    "            writer.add_scalar('dice_loss/', dice_value.item(), iteration_count)\n",
    "            iteration_count+=1  \n",
    "        \n",
    "        # Finished epoch\n",
    "        # Get current learning rate (To display on Tensorboard)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            curr_learning_rate = param_group['lr']\n",
    "            writer.add_scalar('learning_rate/', curr_learning_rate, epoch)\n",
    "        \n",
    "        # Send images from training to tensorboard (Take out)\n",
    "        out_norm = sat_utils.img_minmax_norm_torch(outputs)\n",
    "        labels_norm = sat_utils.img_minmax_norm_torch(labels)\n",
    "        imgs_norm = sat_utils.img_minmax_norm_torch(imgs)\n",
    "        writer.add_images('img_t', imgs_norm, epoch)\n",
    "        writer.add_images('out_mask_t', out_norm, epoch)    \n",
    "        writer.add_images('label_mask_t', labels_norm, epoch)\n",
    "        \n",
    "        # Evaluate model after epoch\n",
    "        model.eval()\n",
    "        list_iou_val = []\n",
    "        list_iou_val_old = []\n",
    "        with torch.no_grad():\n",
    "            for (imgs, labels) in val_loader:\n",
    "                # Send inputs/labels to GPU                \n",
    "                labels = labels.to(device)\n",
    "                imgs = imgs.to(device)                \n",
    "                outputs = model(imgs)\n",
    "            \n",
    "                iou_value = seg_metrics.iou(outputs, labels)\n",
    "                \n",
    "                # Append IoU val for each batch of of data from validation\n",
    "                list_iou_val.append(iou_value.item())\n",
    "\n",
    "        # Report IoU mean \n",
    "        iou_val = np.mean(list_iou_val)\n",
    "        writer.add_scalar('iou_val_run/', np.mean(list_iou_val), epoch)\n",
    "        \n",
    "        # Send images (Take out)\n",
    "        writer.add_images('out_mask_v_run', outputs, epoch)  \n",
    "        writer.add_images('labels_v_run', labels, epoch)\n",
    "        \n",
    "        # Step learning rate scheduler\n",
    "        scheduler_plateau.step(running_loss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Evaluation Function\n",
    "Evaluate the model given:\n",
    "* Some Model\n",
    "* Data Loader\n",
    "This function will run after the end of the train trial, which basically will guide the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, val_loader: DataLoader, dtype: torch.dtype, device: torch.device) -> float:    \n",
    "    global writer \n",
    "    model.eval()\n",
    "    list_iou_val = []\n",
    "    list_iou_val_old = []\n",
    "    with torch.no_grad():\n",
    "        for (imgs, labels) in val_loader:\n",
    "            # Send inputs/labels to GPU                \n",
    "            labels = labels.to(device)\n",
    "            imgs = imgs.to(device)                \n",
    "            outputs = model(imgs)\n",
    "            \n",
    "            iou_value = seg_metrics.iou(outputs, labels)\n",
    "            # Append IoU val for each batch of of data from validation\n",
    "            list_iou_val.append(iou_value.item())\n",
    "    \n",
    "    # Report IoU mean \n",
    "    iou_val = np.mean(list_iou_val)\n",
    "    writer.add_scalar('iou_val/', np.mean(list_iou_val), 0)\n",
    "    \n",
    "    # Send images from validation to tensorboard    \n",
    "    writer.add_images('mask_v', outputs, 0)    \n",
    "    writer.add_images('label_v', labels, 0)\n",
    "\n",
    "    return iou_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define our objective Function\n",
    "This is the function we want to optimize given it's parameters. We want to find the parameters that will maximize accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_func(parameterization):\n",
    "    model = train(train_loader=dataloader_train, val_loader = dataloader_val, \n",
    "                  parameters=parameterization, dtype=torch.float, device=device)\n",
    "    return evaluate(\n",
    "        model=model,\n",
    "        val_loader=dataloader_val,\n",
    "        dtype=torch.float,\n",
    "        device=device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Bayesian Optimizer\n",
    "Search for best learning rate and momentum hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 06-04 15:26:55] ax.service.utils.dispatch: Using Bayesian Optimization generation strategy. Iterations after 5 will take longer to generate due to model-fitting.\n",
      "[INFO 06-04 15:26:55] ax.service.managed_loop: Started full optimization with 40 steps.\n",
      "[INFO 06-04 15:26:55] ax.service.managed_loop: Running optimization trial 1...\n",
      "[INFO 06-04 15:56:22] ax.service.managed_loop: Running optimization trial 2...\n",
      "[INFO 06-04 16:25:18] ax.service.managed_loop: Running optimization trial 3...\n",
      "[INFO 06-04 16:55:07] ax.service.managed_loop: Running optimization trial 4...\n",
      "[INFO 06-04 17:25:10] ax.service.managed_loop: Running optimization trial 5...\n",
      "[INFO 06-04 17:54:11] ax.service.managed_loop: Running optimization trial 6...\n",
      "[INFO 06-04 18:23:43] ax.service.managed_loop: Running optimization trial 7...\n",
      "[INFO 06-04 18:53:38] ax.service.managed_loop: Running optimization trial 8...\n",
      "[INFO 06-04 19:22:56] ax.service.managed_loop: Running optimization trial 9...\n",
      "[INFO 06-04 19:53:01] ax.service.managed_loop: Running optimization trial 10...\n",
      "[INFO 06-04 20:23:08] ax.service.managed_loop: Running optimization trial 11...\n",
      "[INFO 06-04 20:52:56] ax.service.managed_loop: Running optimization trial 12...\n",
      "[INFO 06-04 21:23:22] ax.service.managed_loop: Running optimization trial 13...\n",
      "[INFO 06-04 21:53:25] ax.service.managed_loop: Running optimization trial 14...\n",
      "[INFO 06-04 22:23:08] ax.service.managed_loop: Running optimization trial 15...\n",
      "[INFO 06-04 22:52:43] ax.service.managed_loop: Running optimization trial 16...\n",
      "[INFO 06-04 23:22:19] ax.service.managed_loop: Running optimization trial 17...\n",
      "[INFO 06-04 23:51:55] ax.service.managed_loop: Running optimization trial 18...\n"
     ]
    }
   ],
   "source": [
    "best_parameters, values, experiment, model = optimize(\n",
    "    parameters=[\n",
    "        {\"name\": \"lr\", \"type\": \"range\", \"bounds\": [1e-5, 1e-1], \"log_scale\": True},\n",
    "        {\"name\": \"bce_weight\", \"type\": \"range\", \"bounds\": [0.0, 1.0]},\n",
    "        {\"name\": \"iou_weight\", \"type\": \"range\", \"bounds\": [0.0, 1.0]},\n",
    "        {\"name\": \"dice_weight\", \"type\": \"range\", \"bounds\": [0.0, 1.0]},\n",
    "    ],\n",
    "    evaluation_function=objective_func,\n",
    "    objective_name='IoU',\n",
    "    total_trials=40, # Optional.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'lr': 0.0001800819780227849, 'bce_weight': 0.5371676642028249, 'iou_weight': 0.7814426043284967, 'dice_weight': 0.24227107164197573}\n",
      "IoU: ({'IoU': 0.3398670790417278}, {'IoU': {'IoU': 7.986511373330193e-11}})\n"
     ]
    }
   ],
   "source": [
    "print('Best parameters:',best_parameters)\n",
    "print('IoU:', values)\n",
    "#render(plot_contour(model=model, param_x='lr', param_y='momentum', metric_name='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
