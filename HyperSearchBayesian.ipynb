{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Search\n",
    "Use Bayesian optimization to find (learning-rate and momentum). The advantage of bayesian optimization compared to grid/random search is that those methods are less sample efficient compared to bayesian optimization.\n",
    "\n",
    "#### References\n",
    "* https://en.wikipedia.org/wiki/Bayesian_optimization\n",
    "* https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f\n",
    "* https://ax.dev\n",
    "* https://ax.dev/tutorials/tune_cnn.html\n",
    "* https://botorch.org\n",
    "* https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf\n",
    "* http://krasserm.github.io/2018/03/21/bayesian-optimization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Number of GPUs Available: 8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Dict, Tuple\n",
    "import torch.utils.data as utils\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Library for augmentations on batch of numpy/tensors\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "import sat_utils\n",
    "import seg_loss\n",
    "import seg_metrics\n",
    "import seg_models\n",
    "\n",
    "from ax.plot.contour import plot_contour\n",
    "from ax.plot.trace import optimization_trace_single_method\n",
    "from ax.service.managed_loop import optimize\n",
    "from ax.utils.notebook.plotting import render, init_notebook_plotting\n",
    "from ax.utils.tutorials.cnn_utils import load_mnist, train, evaluate\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = 'cpu'\n",
    "print('Device:', device)\n",
    "num_gpu = torch.cuda.device_count()\n",
    "print('Number of GPUs Available:', num_gpu)\n",
    "\n",
    "lr=1e-6 #0.001 0.0001-(Good with Dice, 0.007 training)\n",
    "l2_norm=0.0000001\n",
    "gamma=0.1\n",
    "batch_size = 64 #32 #20\n",
    "num_epochs = 30\n",
    "step_size = 200\n",
    "\n",
    "writer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data from pickle (Bad not scalable) and create data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_t: (2038, 3, 76, 76)\n",
      "Y_t: (2038, 3, 76, 76)\n",
      "X_v: (227, 3, 76, 76)\n",
      "Y_v: (227, 3, 76, 76)\n"
     ]
    }
   ],
   "source": [
    "X = sat_utils.read_pickle_data('./data/input.pickle')\n",
    "Y = sat_utils.read_pickle_data('./data/label.pickle')\n",
    "\n",
    "# Convert dictionaries to numpy array\n",
    "X = np.stack([sat_utils.get_rgb(x) for x in X.values()])\n",
    "Y = np.stack([(x/255.0) for x in Y.values()])\n",
    "\n",
    "# Split train/validation\n",
    "X_t, X_v, Y_t, Y_v = train_test_split(X, Y, test_size=1/10, random_state=42)\n",
    "print('X_t:', X_t.shape)\n",
    "print('Y_t:', Y_t.shape)\n",
    "print('X_v:', X_v.shape)\n",
    "print('Y_v:', Y_v.shape)\n",
    "\n",
    "# Changes on label for Cross-Entropy (3 classes all mixed on the same image, N,W,H)\n",
    "# Changes on label for BCEWithLogitsLoss (3 classes on 3 Channels, N,C,W,H)\n",
    "tensor_x_t = torch.Tensor(X_t)\n",
    "tensor_y_t = torch.Tensor(Y_t)\n",
    "tensor_x_v = torch.Tensor(X_v)\n",
    "tensor_y_v = torch.Tensor(Y_v)\n",
    "\n",
    "# Define some augmentations\n",
    "seq_augm = iaa.Sequential([\n",
    "    iaa.Fliplr(0.5), # horizontally flip 50% of the images\n",
    "    iaa.Flipud(0.5), # vertically flip 50% of the images\n",
    "    #iaa.Affine(rotate=(-10, 10)), # Rotate the images\n",
    "])\n",
    "\n",
    "dataset_train = utils.TensorDataset(tensor_x_t,tensor_y_t)\n",
    "dataset_val = utils.TensorDataset(tensor_x_v,tensor_y_v)\n",
    "dataloader_train = utils.DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "dataloader_val = utils.DataLoader(dataset_val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "num_classes = tensor_y_t.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Train Function\n",
    "Train the model for one epoch given:\n",
    "* Train loader\n",
    "* Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader: DataLoader, parameters: Dict[str, float], dtype: torch.dtype, device: torch.device) -> nn.Module:\n",
    "    # Start Tensorboard interface\n",
    "    global writer\n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    # Initialize Model and optimizer\n",
    "    model = seg_models.AtrousSeg(num_classes=num_classes, num_channels=tensor_x_t.shape[1])\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    #optimizer = optim.SGD(model.parameters(), lr=parameters.get(\"lr\", 0.001), momentum=parameters.get(\"momentum\", 0.9))\n",
    "    base_lr = parameters.get(\"lr\", 0.001)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=base_lr, weight_decay=l2_norm)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Decrease learning rate if some metric doesnt change for \"patience\" epochs\n",
    "    scheduler_plateau = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=4, verbose=False)\n",
    "    \n",
    "    base_loss_weight = parameters.get(\"loss_weight\", 1.0)\n",
    "    writer.add_scalar('loss_weight/', base_loss_weight, 0)\n",
    "    writer.add_scalar('base_lr/', base_lr, 0)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # Train Network\n",
    "    iteration_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for (imgs, labels) in train_loader:\n",
    "\n",
    "            # Do augmentations the augmentation library expect numpy arrays on format Batch x Row x Cols x Channels\n",
    "            imgs_aug, labels_aug = seq_augm(images=np.moveaxis(imgs.numpy(),1,3), heatmaps=np.moveaxis(labels.numpy(),1,3))\n",
    "\n",
    "            # Move axis back and convert back to tensor\n",
    "            imgs = torch.from_numpy(np.moveaxis(imgs_aug,3,1))\n",
    "            labels = torch.from_numpy(np.moveaxis(labels_aug,3,1))\n",
    "\n",
    "            # Send inputs/labels to GPU                \n",
    "            labels = labels.to(device)\n",
    "            imgs = imgs.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(imgs)\n",
    "            dice_value  = seg_loss.dice_loss(outputs, labels)\n",
    "            loss = (base_loss_weight*criterion(outputs, labels)) + dice_value\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            writer.add_scalar('loss/', loss.item(), iteration_count)\n",
    "            writer.add_scalar('dice_loss/', dice_value.item(), iteration_count)\n",
    "            iteration_count+=1  \n",
    "        \n",
    "        # Finished epoch\n",
    "        # Get current learning rate (To display on Tensorboard)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            curr_learning_rate = param_group['lr']\n",
    "            writer.add_scalar('learning_rate/', curr_learning_rate, epoch)\n",
    "        \n",
    "        # Step learning rate scheduler\n",
    "        scheduler_plateau.step(running_loss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Evaluation Function\n",
    "Evaluate the model given:\n",
    "* Some Model\n",
    "* Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, data_loader: DataLoader, dtype: torch.dtype, device: torch.device) -> float:    \n",
    "    global writer \n",
    "    model.eval()\n",
    "    iteration_count_val = 0\n",
    "    total = 0\n",
    "    iou_val_batch = 0\n",
    "    with torch.no_grad():\n",
    "        for (imgs, labels) in data_loader:\n",
    "            # Send inputs/labels to GPU                \n",
    "            labels = labels.to(device)\n",
    "            imgs = imgs.to(device)                \n",
    "            outputs = model(imgs)\n",
    "            iou_value = seg_metrics.iou(outputs, labels)\n",
    "            iou_val_batch += iou_value.item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    iou_val = iou_val_batch / total\n",
    "    writer.add_scalar('iou_end/', iou_val, 0)\n",
    "\n",
    "    return iou_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define our objective Function\n",
    "This is the function we want to optimize given it's parameters. We want to find the parameters that will maximize accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_func(parameterization):\n",
    "    model = train(train_loader=dataloader_train, parameters=parameterization, dtype=torch.float, device=device)\n",
    "    return evaluate(\n",
    "        model=model,\n",
    "        data_loader=dataloader_val,\n",
    "        dtype=torch.float,\n",
    "        device=device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Bayesian Optimizer\n",
    "Search for best learning rate and momentum hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 05-30 01:08:40] ax.service.utils.dispatch: Using Bayesian Optimization generation strategy. Iterations after 5 will take longer to generate due to model-fitting.\n",
      "[INFO 05-30 01:08:40] ax.service.managed_loop: Started full optimization with 20 steps.\n",
      "[INFO 05-30 01:08:40] ax.service.managed_loop: Running optimization trial 1...\n",
      "[INFO 05-30 01:33:52] ax.service.managed_loop: Running optimization trial 2...\n",
      "[INFO 05-30 01:58:41] ax.service.managed_loop: Running optimization trial 3...\n",
      "[INFO 05-30 02:23:21] ax.service.managed_loop: Running optimization trial 4...\n",
      "[INFO 05-30 02:47:58] ax.service.managed_loop: Running optimization trial 5...\n",
      "[INFO 05-30 03:12:39] ax.service.managed_loop: Running optimization trial 6...\n",
      "[INFO 05-30 03:38:08] ax.service.managed_loop: Running optimization trial 7...\n",
      "[INFO 05-30 04:03:12] ax.service.managed_loop: Running optimization trial 8...\n",
      "[INFO 05-30 04:28:51] ax.service.managed_loop: Running optimization trial 9...\n",
      "[INFO 05-30 04:54:25] ax.service.managed_loop: Running optimization trial 10...\n",
      "[INFO 05-30 05:20:06] ax.service.managed_loop: Running optimization trial 11...\n",
      "[INFO 05-30 05:45:21] ax.service.managed_loop: Running optimization trial 12...\n",
      "[INFO 05-30 06:10:40] ax.service.managed_loop: Running optimization trial 13...\n",
      "/mnt/anaconda3/lib/python3.7/site-packages/gpytorch/utils/cholesky.py:42: RuntimeWarning:\n",
      "\n",
      "A not p.d., added jitter of 1e-08 to the diagonal\n",
      "\n",
      "[INFO 05-30 06:37:04] ax.service.managed_loop: Running optimization trial 14...\n",
      "[INFO 05-30 07:02:45] ax.service.managed_loop: Running optimization trial 15...\n"
     ]
    }
   ],
   "source": [
    "best_parameters, values, experiment, model = optimize(\n",
    "    parameters=[\n",
    "        {\"name\": \"lr\", \"type\": \"range\", \"bounds\": [1e-5, 1e-1], \"log_scale\": True},\n",
    "        {\"name\": \"loss_weight\", \"type\": \"range\", \"bounds\": [0.0, 1.0]},\n",
    "    ],\n",
    "    evaluation_function=objective_func,\n",
    "    objective_name='IoU',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'lr': 0.00010242674403542709, 'loss_weight': 0.43974536824818683}\n",
      "IoU: ({'IoU': 0.006645133133671554}, {'IoU': {'IoU': 2.9800379620516516e-14}})\n"
     ]
    }
   ],
   "source": [
    "print('Best parameters:',best_parameters)\n",
    "print('IoU:', values)\n",
    "#render(plot_contour(model=model, param_x='lr', param_y='momentum', metric_name='accuracy'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
