{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Segmentation with Atrous Convolution\n",
    "\n",
    "\n",
    "#### References\n",
    "* https://arxiv.org/pdf/1709.00179.pdf\n",
    "* https://medium.com/beyondminds/a-simple-guide-to-semantic-segmentation-effcf83e7e54\n",
    "* https://medium.com/dair-ai/medical-imaging-analysis-mri-cnn-pytorch-4877e64e7303\n",
    "* https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7\n",
    "* https://github.com/kevinzakka/pytorch-goodies/blob/master/losses.py\n",
    "* https://arxiv.org/pdf/1702.03275.pdf\n",
    "* https://medium.com/syncedreview/facebook-ai-proposes-group-normalization-alternative-to-batch-normalization-fb0699bffae7\n",
    "* https://www.jeremyjordan.me/evaluating-image-segmentation-models/\n",
    "* https://github.com/martinkersner/py_img_seg_eval\n",
    "* https://medicaltorch.readthedocs.io/en/stable/\n",
    "* https://github.com/meetshah1995/pytorch-semseg\n",
    "* https://discuss.pytorch.org/t/leaf-variable-has-been-moved-into-the-graph-interior/18679/9\n",
    "\n",
    "#### Andrew Ng on Accuracy/Precision/Recall\n",
    "Accuracy it's not important if your dataset is imbalanced (Skewed), for example if your model say 100% of time that someone has no cancer, it will be really accurate, like 99.999% but it's Recall will be zero. \n",
    "* https://www.youtube.com/watch?v=k1JGvqr56Yk&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=66\n",
    "* https://www.youtube.com/watch?v=wGw6R8AbcuI\n",
    "* https://www.youtube.com/watch?v=W5meQnGACGo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "import sat_utils\n",
    "import numpy as np\n",
    "import pickle\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Pytorch stuff\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as utils\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from skimage.filters import threshold_adaptive, threshold_otsu, threshold_local\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "#device = 'cpu'\n",
    "print('Device:', device)\n",
    "\n",
    "lr=0.0001 #0.001\n",
    "l2_norm=0.0000001\n",
    "gamma=0.1\n",
    "batch_size = 20 #20\n",
    "num_epochs = 500\n",
    "step_size = 200\n",
    "SMOOTH = 1e-11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data from pickle (Bad not scalable) and create data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sat_utils.read_pickle_data('./data/input.pickle')\n",
    "Y = sat_utils.read_pickle_data('./data/label.pickle')\n",
    "tensor_x = torch.stack([torch.Tensor(sat_utils.get_rgb(x)) for x in X.values()])\n",
    "\n",
    "# Label from Mean squared error Loss\n",
    "#tensor_y = torch.stack([torch.Tensor(x) for x in Y.values()])\n",
    "tensor_y = torch.stack([torch.Tensor(x/255.0) for x in Y.values()])\n",
    "# Try to make background even more far\n",
    "#tensor_y[tensor_y==tensor_y.min()] = -100.0\n",
    "\n",
    "# Changes on label for Cross-Entropy (3 classes all mixed on the same image, N,W,H)\n",
    "# Changes on label for BCEWithLogitsLoss (3 classes on 3 Channels, N,C,W,H)\n",
    "#tensor_y = torch.stack([torch.Tensor(x/255.0).type(torch.LongTensor) for x in Y.values()])\n",
    "#tensor_y = torch.stack([torch.Tensor(x/255.0).type(torch.FloatTensor) for x in Y.values()])\n",
    "# Just one class\n",
    "#tensor_y[:,2,:,:][tensor_y[:,2,:,:]==2.0] = 0\n",
    "#tensor_y = tensor_y[:,0,:,:] + (tensor_y[:,2,:,:] * 2.0)\n",
    "#print(torch.unique(tensor_y))\n",
    "#print(torch.unique(tensor_y))\n",
    "#tensor_y[:,0,:,:] = 1\n",
    "#tensor_y[:,1,:,:] = 2\n",
    "#tensor_y[:,2,:,:] = 3\n",
    "\n",
    "dataset_train = utils.TensorDataset(tensor_x,tensor_y)\n",
    "dataloader_train = utils.DataLoader(dataset_train, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: torch.Size([40, 3, 76, 76])\n",
      "Label: torch.Size([40, 3, 76, 76])\n",
      "num_classes: 3\n",
      "Max val on label: 1.0\n",
      "Min val on label: 0.0\n"
     ]
    }
   ],
   "source": [
    "print('Input:',tensor_x.shape)\n",
    "print('Label:',tensor_y.shape)\n",
    "num_classes = tensor_y.shape[1]\n",
    "#num_classes = 4\n",
    "print('num_classes:', num_classes)\n",
    "print('Max val on label:', torch.max(tensor_y).item())\n",
    "print('Min val on label:', torch.min(tensor_y).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Loss Functions\n",
    "![alt text](./imgs_doc/metrics.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://gist.github.com/wassname/7793e2058c5c9dacb5212c0ac0b18a8a\n",
    "# https://arxiv.org/pdf/1606.04797v1.pdf\n",
    "def dice_loss(model_outputs, labels):    \n",
    "    smooth = 1.\n",
    "\n",
    "    iflat = model_outputs.view(-1)\n",
    "    tflat = labels.view(-1)\n",
    "    intersection =  torch.abs(iflat * tflat).sum()\n",
    "    \n",
    "    return 1 - ((2. * intersection + smooth) /\n",
    "              ((iflat*iflat).sum() + (tflat*tflat).sum() + smooth))\n",
    "\n",
    "\n",
    "# This version uses hard threshold so it can't be used on the loss function but is precise \n",
    "# to verify the real dice coefficient\n",
    "def dice_val(model_outputs, labels):\n",
    "    #xt = torch.FloatTensor(x,requires_grad=True)\n",
    "    bin_model_outputs = torch.zeros_like(model_outputs, requires_grad=True).type(torch.FloatTensor)\n",
    "    bin_labels = (labels > 0).type(torch.FloatTensor)\n",
    "    \n",
    "    # Convert all channels from model_outputs to binary\n",
    "    list_max = [torch.max(model_outputs[:,ch,:,:]) for ch in range(model_outputs.shape[1])]\n",
    "    list_min = [torch.min(model_outputs[:,ch,:,:]) for ch in range(model_outputs.shape[1])]\n",
    "    list_threshold = [(list_max[ch] - list_min[ch]) / 2.0 for ch in range(model_outputs.shape[1])]\n",
    "    for ch in range(model_outputs.shape[1]):\n",
    "        bin_model_outputs[:,ch,:,:] = model_outputs[:,ch,:,:] > list_threshold[ch]\n",
    "        \n",
    "    smooth = 1.\n",
    "\n",
    "    iflat = bin_model_outputs.view(-1)\n",
    "    tflat = bin_labels.view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "    \n",
    "    return 1 - ((2. * intersection + smooth) /\n",
    "              (iflat.sum() + tflat.sum() + smooth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_outputs, labels format: BATCH x Channels x ROWs x COLs\n",
    "# Reference: https://gist.github.com/wassname/f1452b748efcbeb4cb9b1d059dce6f96\n",
    "# This requires that both labels and model_outputs are on the same range (0..1)\n",
    "def iou_loss(model_outputs, labels):\n",
    "    # Avoid negative values \n",
    "    intersection = torch.abs(model_outputs * labels).sum()\n",
    "    union = torch.abs(model_outputs).sum() + torch.abs(labels).sum()\n",
    "    iou = (intersection + SMOOTH) / (union - intersection + SMOOTH)\n",
    "    # Invert to \"minimize\" to just plug and play on the lost\n",
    "    return 1 - iou\n",
    "\n",
    "# This version uses hard threshold so it can't be used on the loss function but is precise \n",
    "# to verify the real IoU\n",
    "# Reference: https://www.kaggle.com/iezepov/fast-iou-scoring-metric-in-pytorch-and-numpy\n",
    "def iou_val(model_outputs, labels):\n",
    "    bin_model_outputs = torch.zeros_like(model_outputs, requires_grad=False).type(torch.LongTensor)\n",
    "    bin_labels = (labels > 0).type(torch.LongTensor)\n",
    "    # Convert all channels from model_outputs to binary\n",
    "    list_max = [torch.max(model_outputs[:,ch,:,:]) for ch in range(model_outputs.shape[1])]\n",
    "    list_min = [torch.min(model_outputs[:,ch,:,:]) for ch in range(model_outputs.shape[1])]\n",
    "    list_threshold = [(list_max[ch] - list_min[ch]) / 2.0 for ch in range(model_outputs.shape[1])]\n",
    "    for ch in range(model_outputs.shape[1]):\n",
    "        bin_model_outputs[:,ch,:,:] = model_outputs[:,ch,:,:] > list_threshold[ch]\n",
    "    \n",
    "    intersection = (bin_model_outputs & bin_labels).float().sum((2, 3))  # Will be zero if Truth=0 or Prediction=0\n",
    "    union = (bin_model_outputs | bin_labels).float().sum((2, 3))         # Will be zzero if both are 0\n",
    "    # Calculate Intersect over Union (Jaccard Index)\n",
    "    iou = (intersection + SMOOTH) / (union + SMOOTH)  # We smooth our devision to avoid 0/0\n",
    "    return iou.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input 76x76 output 16x16\n",
    "class AtrousSeg(nn.Module):\n",
    "    def __init__(self, num_classes=1, num_channels=8):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            #nn.BatchNorm2d(num_channels),\n",
    "            nn.Conv2d(num_channels, 64, kernel_size=3, stride=1, padding=1, dilation = 1), # Front           \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, dilation = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=0, dilation = 2),            \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=0, dilation = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=0, dilation = 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=0, dilation = 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=0, dilation = 3),            \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            \n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=0, dilation = 3), #LFE\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, dilation = 3), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, dilation = 3), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, dilation = 2), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, dilation = 2), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, dilation = 1), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, dilation = 1),             \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            \n",
    "            nn.Conv2d(256, 1024, kernel_size=7, stride=1, padding=1, dilation = 3), # Head (44x44)\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.Conv2d(1024, 1024, kernel_size=1, stride=1, dilation = 1), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.Conv2d(1024, num_classes, kernel_size=1, stride=1, dilation = 1),\n",
    "            #nn.Sigmoid(),\n",
    "            nn.UpsamplingBilinear2d(size=(76, 76)),\n",
    "        )\n",
    "        # Initialize Weights\n",
    "        for m in self.model:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        result = self.model(x)\n",
    "        # Better for MSE\n",
    "        #return result\n",
    "        # Better for BCEWithLogitsLoss\n",
    "        return F.sigmoid(result)\n",
    "        #return result\n",
    "        #return self.model(x)\n",
    "        if self.training:\n",
    "            return result\n",
    "        else:\n",
    "            return F.sigmoid(result)\n",
    "        #    result = self.model(x)\n",
    "        #    print(x.shape)\n",
    "        #    print(result.shape)\n",
    "        #    F.softmax(result, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AtrousSeg(num_classes=num_classes, num_channels=tensor_x.shape[1])\n",
    "#resp = model(torch.rand(1, 8, 76, 76))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/mnt/anaconda3/lib/python3.7/site-packages/torch/onnx/symbolic.py:173: UserWarning: ONNX export failed on upsample_bilinear2d because align_corners == True not supported\n",
      "  warnings.warn(\"ONNX export failed on \" + op + \" because \" + msg + \" not supported\")\n"
     ]
    }
   ],
   "source": [
    "#writer = SummaryWriter('./logs')\n",
    "# Default directory \"runs\"\n",
    "writer = SummaryWriter()\n",
    "dummy_x = torch.rand(1, tensor_x.shape[1], 76, 76)\n",
    "writer.add_graph(model, dummy_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AtrousSeg(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), dilation=(2, 2))\n",
       "    (7): ReLU()\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), dilation=(2, 2))\n",
       "    (10): ReLU()\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), dilation=(3, 3))\n",
       "    (13): ReLU()\n",
       "    (14): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (15): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), dilation=(3, 3))\n",
       "    (16): ReLU()\n",
       "    (17): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (18): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), dilation=(3, 3))\n",
       "    (19): ReLU()\n",
       "    (20): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (21): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), dilation=(3, 3))\n",
       "    (22): ReLU()\n",
       "    (23): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (24): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(3, 3))\n",
       "    (25): ReLU()\n",
       "    (26): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (27): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(3, 3))\n",
       "    (28): ReLU()\n",
       "    (29): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (30): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(2, 2))\n",
       "    (31): ReLU()\n",
       "    (32): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (33): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(2, 2))\n",
       "    (34): ReLU()\n",
       "    (35): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (36): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (37): ReLU()\n",
       "    (38): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (39): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (40): ReLU()\n",
       "    (41): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (42): Conv2d(256, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(1, 1), dilation=(3, 3))\n",
       "    (43): ReLU()\n",
       "    (44): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (45): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (46): ReLU()\n",
       "    (47): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (48): Conv2d(1024, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (49): UpsamplingBilinear2d(size=(76, 76), mode=bilinear)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Losses and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification losses will have predictions on the format (batch, n_class, 12, 12)\n",
    "# and labels (batch, rows, cols) with values related to indexes of class\n",
    "#loss_fn = nn.CrossEntropyLoss()\n",
    "#loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Regularization losses accept any logits on format (batch, channels, rows, cols) \n",
    "#for both prediction and label\n",
    "loss_fn = nn.MSELoss()\n",
    "#loss_fn = nn.SmoothL1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_norm)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/500 [00:00<?, ?it/s]/mnt/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type AtrousSeg. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "Training:  25%|██▌       | 127/500 [16:01<47:48,  7.69s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-14f6da9c32e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'out_mask'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_norm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'out_between'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_norm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'out_border'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_norm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/anaconda3/lib/python3.7/site-packages/torch/utils/tensorboard/writer.py\u001b[0m in \u001b[0;36madd_images\u001b[0;34m(self, tag, img_tensor, global_step, walltime, dataformats)\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0mimg_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworkspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFetchBlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         self._get_file_writer().add_summary(\n\u001b[0;32m--> 404\u001b[0;31m             image(tag, img_tensor, dataformats=dataformats), global_step, walltime)\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     def add_image_with_boxes(self, tag, img_tensor, box_tensor, global_step=None,\n",
      "\u001b[0;32m/mnt/anaconda3/lib/python3.7/site-packages/torch/utils/tensorboard/summary.py\u001b[0m in \u001b[0;36mimage\u001b[0;34m(tag, tensor, rescale, dataformats)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_clean_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_HWC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataformats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0;31m# Do not assume that user passes in values in [0, 255], use data type to detect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0mscale_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_calc_scale_factor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/anaconda3/lib/python3.7/site-packages/torch/utils/tensorboard/_utils.py\u001b[0m in \u001b[0;36mconvert_to_HWC\u001b[0;34m(tensor, input_format)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minput_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m'NCHW'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mtensor_NCHW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mtensor_CHW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_NCHW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_CHW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/anaconda3/lib/python3.7/site-packages/torch/utils/tensorboard/_utils.py\u001b[0m in \u001b[0;36mmake_grid\u001b[0;34m(I, ncols)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mnimg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mcanvas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcanvas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iteration_count = 0\n",
    "# For all epochs\n",
    "for epoch in tqdm(range(num_epochs), desc='Training'):\n",
    "    # Train step\n",
    "    model.train()    \n",
    "    # For all elements on the training set\n",
    "    for i, (imgs, labels) in enumerate(dataloader_train):\n",
    "        # Send inputs/labels to GPU                \n",
    "        labels = labels.to(device)\n",
    "        imgs = imgs.to(device)                \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(imgs)\n",
    "        \n",
    "        #loss_dice = dice_loss(outputs, labels)\n",
    "        iou_value = iou_loss(outputs, labels)\n",
    "        dice_value  = dice_loss(outputs, labels)\n",
    "        #print(type(dice_val))\n",
    "        #loss = loss_fn(outputs, labels) + (1000*dice_val)\n",
    "        #loss = dice_val\n",
    "        loss = iou_value\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        exp_lr_scheduler.step(epoch)\n",
    "        writer.add_scalar('loss/', loss.item(), iteration_count)\n",
    "        #writer.add_scalar('loss_dice/', loss_dice.item(), iteration_count)\n",
    "        iteration_count+=1        \n",
    "    #print('Epoch:', epoch, 'of:', num_epochs, 'loss:', loss.item())\n",
    "    # Get number of channels on output (Number of classes)\n",
    "    num_channels_outputs = outputs.shape[1]\n",
    "    \n",
    "    # Get Iou, Dice\n",
    "    iou_val = iou_loss(outputs, labels)\n",
    "    writer.add_scalar('iou/', iou_value.item(), epoch)\n",
    "    dice_val = dice_loss(outputs, labels)\n",
    "    writer.add_scalar('dice/', dice_value.item(), epoch)\n",
    "    \n",
    "    # Get current learning rate (To display on Tensorboard)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        curr_learning_rate = param_group['lr']\n",
    "        writer.add_scalar('learning_rate/', curr_learning_rate, epoch)\n",
    "    \n",
    "    # Send to tensorboard loss\n",
    "    out_norm = sat_utils.img_minmax_norm_torch(outputs)\n",
    "    labels_norm = sat_utils.img_minmax_norm_torch(labels)\n",
    "    imgs_norm = sat_utils.img_minmax_norm_torch(imgs)\n",
    "    writer.add_images('Image', imgs_norm, epoch)\n",
    "    if num_classes > 1:\n",
    "        writer.add_images('out_mask', out_norm[:, 0, :, :].unsqueeze(1), epoch)    \n",
    "        writer.add_images('out_between', out_norm[:, 1, :, :].unsqueeze(1), epoch)\n",
    "        writer.add_images('out_border', out_norm[:, 2, :, :].unsqueeze(1), epoch)\n",
    "        if len(labels_norm.shape) > 3:\n",
    "            writer.add_images('label_mask', labels_norm[:, 0, :, :].unsqueeze(1), epoch)\n",
    "            writer.add_images('label_between', labels_norm[:, 1, :, :].unsqueeze(1), epoch)\n",
    "            writer.add_images('label_border', labels_norm[:, 2, :, :].unsqueeze(1), epoch)\n",
    "        else:\n",
    "            writer.add_images('label_mask', labels_norm.unsqueeze(1), epoch)  \n",
    "    else:\n",
    "        writer.add_images('out_mask', out_norm, epoch)            \n",
    "        writer.add_images('label_mask', labels_norm.unsqueeze(1), epoch)  \n",
    "    \n",
    "    # Save Model\n",
    "    torch.save(model, './model_save/model_'+str(epoch)+'.cpkt')\n",
    "    \n",
    "    #img_idx = randint(0, batch_size-1)\n",
    "    #img_input = imgs_norm[img_idx,:,:,:].cpu().numpy()\n",
    "    #f, axarr = plt.subplots(1, (num_channels_outputs*2) + 1, figsize=(15,15))\n",
    "    #axarr[0].imshow(np.moveaxis(img_input, 0, 2)) #4 With 8 channels\n",
    "    #axarr[1].imshow(outputs[img_idx,0,:,:].detach().cpu())\n",
    "    #axarr[2].imshow(outputs[img_idx,1,:,:].detach().cpu())\n",
    "    #axarr[3].imshow(outputs[img_idx,2,:,:].detach().cpu())    \n",
    "    #axarr[4].imshow(labels[img_idx,0,:,:].cpu())\n",
    "    #axarr[5].imshow(labels[img_idx,1,:,:].cpu())\n",
    "    #axarr[6].imshow(labels[img_idx,2,:,:].cpu())\n",
    "    #***-----------plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Specific Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AtrousSeg(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), dilation=(2, 2))\n",
       "    (7): ReLU()\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), dilation=(2, 2))\n",
       "    (10): ReLU()\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), dilation=(3, 3))\n",
       "    (13): ReLU()\n",
       "    (14): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (15): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), dilation=(3, 3))\n",
       "    (16): ReLU()\n",
       "    (17): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (18): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), dilation=(3, 3))\n",
       "    (19): ReLU()\n",
       "    (20): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (21): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), dilation=(3, 3))\n",
       "    (22): ReLU()\n",
       "    (23): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (24): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(3, 3))\n",
       "    (25): ReLU()\n",
       "    (26): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (27): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(3, 3))\n",
       "    (28): ReLU()\n",
       "    (29): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (30): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(2, 2))\n",
       "    (31): ReLU()\n",
       "    (32): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (33): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(2, 2))\n",
       "    (34): ReLU()\n",
       "    (35): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (36): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (37): ReLU()\n",
       "    (38): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (39): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (40): ReLU()\n",
       "    (41): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (42): Conv2d(256, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(1, 1), dilation=(3, 3))\n",
       "    (43): ReLU()\n",
       "    (44): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (45): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (46): ReLU()\n",
       "    (47): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (48): Conv2d(1024, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (49): UpsamplingBilinear2d(size=(76, 76), mode=bilinear)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('./model_save/model_126.cpkt')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f74c907df1ff47418d25eda1573fd436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='idx_img', max=39), Output()), _dom_classes=('widget-inte…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(idx_img=widgets.IntSlider(min=0,max=tensor_x.shape[0]-1), th_mask_iteractive=widgets.IntSlider(min=0,max=100), use_threshold = False)\n",
    "def testModel(idx_img):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        img = tensor_x[idx_img].unsqueeze(0).to(device)\n",
    "        pred = model(img)\n",
    "        label = tensor_y[idx_img].to(device)\n",
    "        dice_value = dice_val(pred, label.unsqueeze(0))\n",
    "        iou_value = iou_val(pred, label.unsqueeze(0))\n",
    "        dice_loss_value = dice_loss(pred, label.unsqueeze(0))\n",
    "        iou_loss_value = iou_loss(pred, label.unsqueeze(0))\n",
    "        \n",
    "    img_numpy = img.cpu().squeeze().numpy()\n",
    "    img_numpy = sat_utils.img_minmax_norm(img_numpy)\n",
    "    img_numpy = np.moveaxis(img_numpy, 0, 2)\n",
    "    pred_numpy = pred.cpu().squeeze().numpy()\n",
    "    # Test\n",
    "    pred_numpy_sig = F.sigmoid(pred).cpu().squeeze().numpy()\n",
    "    label_numpy = tensor_y[idx_img].squeeze().numpy()   \n",
    "    img_numpy = sat_utils.img_minmax_norm(img_numpy)\n",
    "    \n",
    "    print('Dice Val:', dice_value)\n",
    "    print('IoU Val:', iou_value)\n",
    "    print('Dice Loss:', dice_loss_value)\n",
    "    print('IoU Loss:', iou_loss_value)\n",
    "    \n",
    "    if num_classes > 1:\n",
    "        # Merge Mask and Border\n",
    "        mask_border = pred_numpy[0,:,:] - pred_numpy[1,:,:]\n",
    "        \n",
    "        f, axarr = plt.subplots(1, 5, figsize=(15,15))\n",
    "        axarr[0].imshow(img_numpy) #4 With 8 channels\n",
    "        axarr[0].title.set_text('Original')\n",
    "        axarr[1].imshow(pred_numpy[0,:,:])\n",
    "        axarr[1].title.set_text('Prediction Mask')\n",
    "        axarr[2].imshow(pred_numpy[1,:,:])\n",
    "        axarr[2].title.set_text('Prediction Border')\n",
    "        axarr[3].imshow(mask_border)\n",
    "        axarr[3].title.set_text('Subtracted')\n",
    "        axarr[4].imshow(label_numpy[0,:,:])\n",
    "        axarr[4].title.set_text('Label')\n",
    "    else:\n",
    "        f, axarr = plt.subplots(1, 3, figsize=(15,15))\n",
    "        axarr[0].imshow(img_numpy[0,:,:]) #4 With 8 channels\n",
    "        axarr[0].title.set_text('Original')\n",
    "        axarr[1].imshow(pred_numpy[:,:])\n",
    "        axarr[1].title.set_text('Prediction Mask')\n",
    "        axarr[2].imshow(label_numpy)\n",
    "        axarr[2].title.set_text('Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
